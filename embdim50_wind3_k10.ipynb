{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F1\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "window_size =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#print(type(reuters))\n",
    "\n",
    "documents = reuters.fileids()\n",
    "#print(str(len(documents)) + \" documents\");\n",
    "\n",
    "train_docs = list(filter(lambda doc: doc.startswith(\"train\"),\n",
    "                        documents));\n",
    "#print(str(len(train_docs)) + \" total train documents\");\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "test_docs = list(filter(lambda doc: doc.startswith(\"test\"),\n",
    "                       documents));\n",
    "#print(str(len(test_docs)) + \" total test documents\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25033\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "count1=0\n",
    "tokenized_corpus=[]\n",
    "for id in train_docs:\n",
    "    for sentence in reuters.sents(id):\n",
    "        for i in range(len(sentence)):\n",
    "            sentence[i]=sentence[i].lower()\n",
    "            if sentence[i].isnumeric():\n",
    "                sentence[i]=\"num\"\n",
    "        tokenized_corpus.append(sentence[:-1])\n",
    "          \n",
    "\n",
    "    count=count+1\n",
    "\n",
    "#corpus is now tokenized\n",
    "\n",
    "vocabulary = []\n",
    "tokens=[]\n",
    "for sentence in tokenized_corpus:\n",
    "    \n",
    "    \n",
    "    for token in sentence:\n",
    "        tokens.append(token)\n",
    "        #if token.isnumeric():\n",
    "            #tokens.append(\"num\")\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "#print(vocabulary)\n",
    "\n",
    "#print(vocabulary[0:10])\n",
    "print(len(vocabulary))\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pickle_out = open(\"pickles/vocab.pickle\",\"wb\")\n",
    "pickle.dump(vocabulary, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6797984 :idx_pairs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "print (len(idx_pairs),\":idx_pairs\")\n",
    "idx_pairs1=idx_pairs[int(len(idx_pairs)/10):,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"pickles/word2id.pickle\",\"wb\")\n",
    "pickle.dump(word2idx, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"pickles/id2word.pickle\",\"wb\")\n",
    "pickle.dump(idx2word, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = np.zeros(vocabulary_size)\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "from numba import vectorize\n",
    "#@vectorize(['float32(float32, float32, float32,float32,float32)',\n",
    "#            'float64(float64, float64, float64,float64,float64)'],\n",
    "#           target='roc')\n",
    "@jit()\n",
    "def train(i,li,nli,W1,W2):\n",
    "    start_time=time.time()\n",
    "    for j in li:\n",
    "        dot=np.dot(W2[j],W1[i])\n",
    "        #e=math.exp(-dot)\n",
    "        if(dot)>=9 :\n",
    "            W1[i]=W1[i]\n",
    "            W2[j]=W2[j]\n",
    "\n",
    "        elif (dot)>=-8 and (dot)<9:\n",
    "            e=math.exp(-dot)\n",
    "            W1[i]=W1[i]+learning_rate*W2[j]*(e)/(1+e)\n",
    "            W2[j]=W2[j]+learning_rate*W1[i]*(e)/(1+e)\n",
    "        else:\n",
    "            W1[i]=W1[i]+learning_rate*W2[j]\n",
    "            W2[j]=W2[j]+learning_rate*W1[i]                                                             \n",
    "\n",
    "\n",
    "                                                                               \n",
    "                                                                               \n",
    "    for j in nli:\n",
    "        dot=np.dot(W2[j],W1[i])                                                                   \n",
    "                                                                           \n",
    "        if(dot)<=-9 :\n",
    "            W1[i]=W1[i]\n",
    "            W2[j]=W2[j]  #switched\n",
    "              \n",
    "        elif(dot<8 and dot>-9):\n",
    "            e=math.exp(dot)\n",
    "            W1[i]=W1[i]-learning_rate*W2[j]* e/(1+e)\n",
    "            W2[j]=W2[j]-learning_rate*W1[i]*e/(1+e)\n",
    "        else:\n",
    "\n",
    "            W1[i]=W1[i]-learning_rate*W2[j]\n",
    "            W2[j]=W2[j]-learning_rate*W1[i]  \n",
    "    #print(\"time in one train:\",time.time()-start_time)\n",
    "    return(W1,W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "time in this epoch: 1173.5726716518402\n",
      "epoch: 1\n",
      "time in this epoch: 1123.36700797081\n",
      "epoch: 2\n",
      "time in this epoch: 1343.5374248027802\n",
      "epoch: 3\n",
      "time in this epoch: 1347.9827642440796\n",
      "epoch: 4\n",
      "time in this epoch: 1348.8725755214691\n",
      "epoch: 5\n",
      "time in this epoch: 1287.9724769592285\n",
      "epoch: 6\n",
      "time in this epoch: 1192.4985496997833\n",
      "epoch: 7\n",
      "time in this epoch: 939.7079894542694\n",
      "epoch: 8\n",
      "time in this epoch: 1025.9289541244507\n",
      "epoch: 9\n",
      "time in this epoch: 1072.7181375026703\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 50\n",
    "import math\n",
    "#W1=np.random.randint(2,size=(embedding_dims,vocabulary_size))\n",
    "#W2=np.random.randint(2,size=(vocabulary_size,embedding_dims))\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "W1=np.random.rand(embedding_dims,vocabulary_size) \n",
    "W2=np.random.rand(vocabulary_size,embedding_dims) \n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "W1=W1.T\n",
    "for epo in range(num_epochs):\n",
    "    m=0\n",
    "    ste=time.time()\n",
    "    print(\"epoch:\",epo)\n",
    "    loss_val = 0\n",
    "    \n",
    "    #for data in vocabulary:\n",
    "    li=[]\n",
    "    nli=[]\n",
    "       \n",
    "    start_time = time.time()\n",
    "    for input1,target in idx_pairs:\n",
    "\n",
    "\n",
    "        li=[target]\n",
    "        nli=list()\n",
    "        i=input1\n",
    "\n",
    "        c=0\n",
    "        st=time.time()\n",
    "        while(c<k):\n",
    "            r=random.randint(0,len(tokens)-1)\n",
    "            if word2idx[tokens[r]] not in nli:\n",
    "                nli.append(word2idx[tokens[r]])\n",
    "                c+=1\n",
    "            \n",
    "\n",
    "       \n",
    "        W1,W2=train(i,li,nli,W1,W2)\n",
    "\n",
    "        W=W1\n",
    "\n",
    "\n",
    "    np.savetxt('data/w_embdim50_wind3_k10.txt',W)\n",
    "    print(\"time in this epoch:\",time.time()-ste)\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15628857"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "p=np.loadtxt('data/w_embdim50_wind3_k10.txt')\n",
    "\n",
    "pickle_out = open(\"pickles/embedding.pickle\",\"wb\")\n",
    "pickle.dump(p, pickle_out)\n",
    "pickle_out.close()\n",
    "str1=\"\"\n",
    "for i in range(len(vocabulary)):\n",
    "    str1=str1+vocabulary[i]\n",
    "\n",
    "    str1=str1+\" \"+str(p[i])[1:-1]+\"\\n\"\n",
    "\n",
    "text_file = open(\"data/w_embdim50_wind3_k10_with_words.txt\", \"w+\")\n",
    "\n",
    "text_file.write(str1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss calculation for test\n",
    "test_corpus=[]\n",
    "for id in test_docs:\n",
    "    for sentence in reuters.sents(id):\n",
    "        for i in range(len(sentence)):\n",
    "            sentence[i]=sentence[i].lower()\n",
    "            if sentence[i].isnumeric():\n",
    "                sentence[i]=\"num\"\n",
    "        test_corpus.append(sentence[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15820\n"
     ]
    }
   ],
   "source": [
    "test_vocabulary = []\n",
    "test_tokens=[]\n",
    "for sentence in test_corpus:\n",
    "    \n",
    "    \n",
    "    for token in sentence:\n",
    "        test_tokens.append(token)\n",
    "\n",
    "        if token not in test_vocabulary:\n",
    "            test_vocabulary.append(token)\n",
    "\n",
    "test_word2idx = {w: idx for (idx, w) in enumerate(test_vocabulary)}\n",
    "test_idx2word = {idx: w for (idx, w) in enumerate(test_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in test_corpus:\n",
    "    indices = [test_word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            test_idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "test_idx_pairs = np.array(test_idx_pairs) # it will be useful to have this as numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=0\n",
    "for input1,target in test_idx_pairs:\n",
    "    c=0\n",
    "    nli=[]\n",
    "    st=time.time()\n",
    "    while(c<k):\n",
    "        r=random.randint(0,len(tokens)-1)\n",
    "            #if(i,word2idx[tokens[r]]) not in idx_pairs :\n",
    "        if word2idx[tokens[r]] not in nli:\n",
    "            nli.append(word2idx[tokens[r]])\n",
    "            c+=1\n",
    "    if(test_idx2word[input1] not in vocabulary):\n",
    "        v1=np.random.rand(embedding_dims,1)\n",
    "    else:\n",
    "        v1=W1[word2idx[test_idx2word[input1]]].reshape(embedding_dims,1)\n",
    "    if(test_idx2word[target] not in vocabulary):\n",
    "        v2=np.random.rand(embedding_dims,1)\n",
    "    else:\n",
    "        v2=W2[word2idx[test_idx2word[target]]].reshape(embedding_dims,1)\n",
    "    #print (v1.shape,v2.shape)\n",
    "    #dot=np.dot(v1.reshape(embedding_dims,1).T,v2.reshape(embedding_dims,1))\n",
    "    dot=np.dot(v1.T,v2)\n",
    "    loss=-math.log(1/(1+math.exp(-dot)))\n",
    "    \n",
    "    for i in range(k):\n",
    "        dot=np.dot(v1.T,W2[nli[i]])\n",
    "        loss=loss-math.log(1/(1+math.exp(dot)))\n",
    "    loss=loss/(len(test_idx_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5160957291528692e-06\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(v1,v2):\n",
    "    return np.dot(v1,v2)/math.sqrt(np.dot(v1,v1)*np.dot(v2,v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word1        word2  SimLex999\n",
      "0    old          new       1.58\n",
      "1  smart  intelligent       9.20\n",
      "2   hard    difficult       8.77\n",
      "3  happy     cheerful       9.55\n",
      "4   hard         easy       0.95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/SimLex-999.txt\",sep=\"\\t\")\n",
    "\n",
    "df1 = df[['word1', 'word2','SimLex999']]\n",
    "\n",
    "print (df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.06060212e-01  4.74818871e-01  6.31041949e-01  0.00000000e+00\n",
      " -1.16579636e-01  5.22905728e-01  4.00856756e-01  8.24988317e-01\n",
      "  0.00000000e+00  0.00000000e+00  4.62828603e-01 -3.14408486e-01\n",
      " -5.87076267e-02 -2.84620970e-01 -3.59229085e-01  0.00000000e+00\n",
      "  6.76515252e-01  3.09056022e-01  6.48416940e-01  4.60064559e-01\n",
      "  6.10715611e-01 -4.36461413e-01  2.48321273e-01  0.00000000e+00\n",
      "  0.00000000e+00  6.67520545e-01  2.06537086e-01  5.20220777e-01\n",
      "  4.74717596e-01  5.85929852e-01  2.49694126e-02 -4.97630780e-02\n",
      "  5.99235858e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -2.26695093e-01  0.00000000e+00  0.00000000e+00\n",
      "  6.42531482e-01  0.00000000e+00  6.93011211e-01  0.00000000e+00\n",
      "  6.59409235e-01  5.89353785e-01 -4.21896718e-01 -2.06183852e-01\n",
      "  0.00000000e+00  0.00000000e+00  4.77786319e-01 -3.18144656e-01\n",
      "  7.17115183e-01  0.00000000e+00  7.36403053e-01  0.00000000e+00\n",
      "  9.53181540e-03  8.37739224e-02  4.71583091e-01  1.91980127e-02\n",
      "  5.63741411e-01  0.00000000e+00  1.70399211e-01  0.00000000e+00\n",
      "  0.00000000e+00  6.04403907e-01  1.31667745e-01  6.81539084e-01\n",
      "  6.06010355e-01  6.64360889e-01  6.63187606e-01  0.00000000e+00\n",
      "  0.00000000e+00 -2.18552692e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  3.35643393e-01  1.20649905e-02 -2.06723712e-01\n",
      " -2.86029827e-01  0.00000000e+00  0.00000000e+00  5.11229458e-01\n",
      "  0.00000000e+00  0.00000000e+00  5.58185358e-01  5.88818410e-01\n",
      "  9.34157559e-03  0.00000000e+00  0.00000000e+00  7.49530809e-01\n",
      "  7.96105316e-01  6.88324704e-01  7.10307641e-01 -1.80357149e-01\n",
      "  0.00000000e+00  0.00000000e+00 -3.96093313e-01  5.94539262e-01\n",
      "  7.32973887e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.67534630e-01  5.55440439e-01\n",
      "  0.00000000e+00  5.13436252e-01  0.00000000e+00  0.00000000e+00\n",
      " -1.42330269e-01  0.00000000e+00  5.16356513e-01  6.60741540e-01\n",
      "  4.13465515e-01  0.00000000e+00  0.00000000e+00  3.69233275e-01\n",
      "  0.00000000e+00  0.00000000e+00 -5.18970693e-02  5.39939888e-03\n",
      "  6.15187764e-01  0.00000000e+00  6.82805436e-01  0.00000000e+00\n",
      "  0.00000000e+00  2.98255962e-01  5.24475900e-01  0.00000000e+00\n",
      " -2.16363819e-02  7.55917260e-01  6.84581114e-01 -5.02587610e-02\n",
      "  0.00000000e+00  0.00000000e+00  7.60568976e-01  0.00000000e+00\n",
      " -2.63378473e-01 -3.88748453e-02  6.88864171e-01  0.00000000e+00\n",
      "  6.80816522e-01  0.00000000e+00 -2.82538294e-01 -5.25859357e-02\n",
      "  8.32831431e-02  5.97854939e-01  0.00000000e+00  5.30575548e-01\n",
      "  0.00000000e+00  0.00000000e+00  4.85512099e-02  0.00000000e+00\n",
      "  4.80075814e-01  2.52301428e-02 -8.45470023e-02  0.00000000e+00\n",
      "  0.00000000e+00  7.10538474e-01  0.00000000e+00 -1.11744894e-01\n",
      "  7.75001903e-01  7.30055308e-01  0.00000000e+00  0.00000000e+00\n",
      "  5.17468492e-01  4.20636033e-01  6.45469364e-01  7.85019866e-01\n",
      "  0.00000000e+00  0.00000000e+00  5.28570881e-01  6.32523645e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  6.83034045e-01\n",
      " -2.90623074e-01  0.00000000e+00  5.33102386e-01  0.00000000e+00\n",
      "  3.18664532e-01  0.00000000e+00  7.18273979e-01  0.00000000e+00\n",
      "  0.00000000e+00  5.96039771e-01  7.03966941e-01 -2.20766965e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  7.09434298e-01\n",
      "  5.77045560e-01  2.81303165e-01  0.00000000e+00  7.46810431e-01\n",
      "  3.65597483e-01  0.00000000e+00 -3.66420468e-01  7.30842745e-02\n",
      "  0.00000000e+00  0.00000000e+00  6.15014907e-01  8.19844786e-02\n",
      "  0.00000000e+00 -1.15806382e-02  7.69816027e-01  1.78448373e-01\n",
      "  6.65462674e-01  0.00000000e+00  3.30463923e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.89329511e-01\n",
      " -2.69272954e-01 -1.76610630e-02  0.00000000e+00  5.67059555e-01\n",
      " -1.12337350e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  3.76062436e-01  7.38812359e-01  7.98442540e-01  0.00000000e+00\n",
      "  2.34953800e-01  6.05807162e-01  7.00721474e-01  5.56172013e-01\n",
      "  6.97183098e-01  7.12271562e-01  0.00000000e+00  5.17599676e-01\n",
      "  2.47257694e-01  0.00000000e+00  3.98544595e-01  0.00000000e+00\n",
      "  0.00000000e+00 -2.27914925e-03  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.70268470e-01  0.00000000e+00\n",
      "  7.88275905e-01 -2.23745655e-01  0.00000000e+00  8.51792587e-02\n",
      "  7.61942307e-01  4.02366929e-01  0.00000000e+00  2.38835570e-01\n",
      "  6.82219951e-01  4.41846320e-01  1.81258899e-01  0.00000000e+00\n",
      "  0.00000000e+00  7.15865721e-01  7.32866146e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.00758292e-01  4.68589956e-01\n",
      "  0.00000000e+00  0.00000000e+00  6.18917390e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.22381519e-01  0.00000000e+00\n",
      "  3.14146000e-01  3.26854580e-01  5.24813396e-01  0.00000000e+00\n",
      "  5.95270385e-01 -2.42738739e-02  0.00000000e+00  5.91876600e-02\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.80749273e-01  0.00000000e+00  6.31395872e-01\n",
      "  5.81697372e-01  5.62136376e-01  5.58566627e-01  1.56100495e-01\n",
      " -2.26948633e-01  6.19178954e-01  0.00000000e+00  0.00000000e+00\n",
      " -1.86331822e-01  0.00000000e+00  0.00000000e+00  3.65196960e-01\n",
      "  0.00000000e+00  5.71351575e-01  0.00000000e+00 -1.39446330e-02\n",
      "  7.35576709e-01  6.22868806e-01 -1.62206774e-01  5.40247921e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.94157730e-01\n",
      "  7.45809916e-01  0.00000000e+00  7.68853028e-01 -3.02218262e-01\n",
      "  0.00000000e+00  4.55708830e-01  0.00000000e+00  6.75791561e-01\n",
      "  1.43532953e-01  0.00000000e+00 -1.35255843e-01  2.33476497e-02\n",
      "  6.17113717e-01  0.00000000e+00  5.53683879e-01  6.60807083e-01\n",
      "  0.00000000e+00 -2.23561581e-01  5.03793461e-01  0.00000000e+00\n",
      "  6.15801970e-02  2.74496207e-01  5.81065892e-01  7.22999713e-01\n",
      "  5.81311180e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  4.96660497e-01  0.00000000e+00  3.70748828e-01  4.34258867e-01\n",
      "  7.05904073e-01  7.01803204e-01  2.11511793e-01  0.00000000e+00\n",
      "  6.62440435e-01  7.70783908e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.73230417e-01\n",
      "  0.00000000e+00 -1.31058120e-01  5.95537784e-02  6.99979710e-01\n",
      "  0.00000000e+00 -1.40117954e-01  0.00000000e+00 -1.36926956e-01\n",
      "  3.70382824e-01 -1.77575376e-01  0.00000000e+00  2.87712677e-01\n",
      " -2.88488430e-01  1.75710472e-01  0.00000000e+00  0.00000000e+00\n",
      " -4.12477827e-01  6.69801353e-01  0.00000000e+00  8.24865063e-02\n",
      "  0.00000000e+00  6.09697849e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  5.13302980e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.56677539e-01\n",
      "  4.89610414e-01  0.00000000e+00  0.00000000e+00  4.57770651e-01\n",
      "  2.07790344e-01 -1.79468589e-01  0.00000000e+00  6.26724940e-01\n",
      " -1.98346595e-01  0.00000000e+00  0.00000000e+00  3.41651274e-01\n",
      "  6.62575469e-01  6.81068629e-01  0.00000000e+00  0.00000000e+00\n",
      "  7.30110815e-01  7.10397044e-01  0.00000000e+00 -1.55890622e-01\n",
      "  3.83761476e-02  6.52578864e-01  0.00000000e+00 -1.79544893e-01\n",
      "  7.82084124e-01  5.94102152e-01  4.48716209e-02  6.39370367e-01\n",
      "  2.08673226e-01  0.00000000e+00 -3.41246336e-01  0.00000000e+00\n",
      "  7.36831542e-01  0.00000000e+00  2.90426552e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -8.86662554e-02\n",
      "  7.08428456e-01 -2.24865507e-01  4.30122582e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  7.23876746e-01\n",
      "  0.00000000e+00  5.43753683e-01  1.26251154e-01  6.42663728e-01\n",
      "  0.00000000e+00  6.73151489e-01  0.00000000e+00 -3.27933531e-01\n",
      "  1.21531879e-01  5.44105256e-01  5.04463617e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  5.41739688e-01\n",
      " -3.39943337e-01 -1.97801475e-01 -8.92232675e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  3.15576417e-01 -1.91820511e-01\n",
      "  7.33197655e-01  0.00000000e+00  0.00000000e+00  3.51941098e-01\n",
      " -6.99261769e-02  7.29040728e-01 -3.08432466e-01  9.78584232e-02\n",
      "  0.00000000e+00  4.25953150e-01  3.18818320e-02  5.13017080e-01\n",
      " -6.88772019e-02 -1.32572045e-01  4.69847654e-01  6.24750518e-01\n",
      " -3.98744985e-01  5.10745518e-01  0.00000000e+00  0.00000000e+00\n",
      " -2.41077610e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -2.14224992e-01 -1.53378977e-01  2.61531592e-01\n",
      " -2.05483535e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  6.31970249e-01  7.65575876e-01\n",
      "  5.61383257e-01  0.00000000e+00  0.00000000e+00 -3.57742348e-03\n",
      "  2.71044752e-01  3.49502234e-01  0.00000000e+00  8.07405148e-01\n",
      "  1.17022780e-01 -2.51041064e-01 -1.71473291e-01  0.00000000e+00\n",
      "  3.39228781e-01 -2.26821265e-01 -1.70073707e-01  0.00000000e+00\n",
      "  0.00000000e+00  4.69542337e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  4.90142735e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -2.21662944e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  8.15833757e-01  4.84932851e-01\n",
      "  4.48142453e-01  1.76477868e-01 -1.53446535e-01  6.55996925e-01\n",
      "  3.17434143e-01  4.73966885e-01  6.25186657e-01  0.00000000e+00\n",
      "  5.98857116e-01  0.00000000e+00 -3.09434036e-01  3.08324390e-01\n",
      "  6.20580718e-01  0.00000000e+00 -3.35030120e-01 -3.07209767e-01\n",
      "  1.86098735e-01  1.66357841e-01  0.00000000e+00  2.76462197e-01\n",
      "  0.00000000e+00  5.83065414e-02  0.00000000e+00  0.00000000e+00\n",
      "  7.13210025e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  3.06201554e-01  4.47791557e-01  0.00000000e+00  0.00000000e+00\n",
      "  4.06663946e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.92256741e-01  0.00000000e+00  2.58855981e-01  0.00000000e+00\n",
      "  0.00000000e+00  6.85205387e-01  6.71967970e-01 -1.86259239e-01\n",
      "  3.81625145e-01  4.09586337e-01  2.73092875e-01  0.00000000e+00\n",
      " -1.98515906e-02  5.78028207e-01  2.81587180e-01  2.12285598e-01\n",
      " -1.38073621e-01  5.48015960e-01  0.00000000e+00  0.00000000e+00\n",
      "  6.39189842e-01  7.68261585e-01 -6.00980907e-02 -1.63205368e-01\n",
      "  0.00000000e+00  6.03871638e-01  0.00000000e+00  0.00000000e+00\n",
      "  6.02179863e-01 -2.39701528e-01  0.00000000e+00  0.00000000e+00\n",
      " -7.79988694e-02 -2.27546836e-01  0.00000000e+00 -5.18009034e-02\n",
      "  4.01661371e-01  0.00000000e+00  0.00000000e+00 -1.12050751e-01\n",
      "  0.00000000e+00 -7.98570300e-04  3.19486136e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  4.04330921e-01 -2.15654523e-02  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.75563481e-01  0.00000000e+00  0.00000000e+00  6.96353765e-01\n",
      "  0.00000000e+00  0.00000000e+00  6.29385452e-01  0.00000000e+00\n",
      "  0.00000000e+00  7.49051454e-01  0.00000000e+00  0.00000000e+00\n",
      "  4.58175551e-01  0.00000000e+00 -4.79745013e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  7.17069135e-01  0.00000000e+00\n",
      "  4.70989399e-01  0.00000000e+00 -2.38631227e-01  8.18753814e-01\n",
      " -3.09165207e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -2.60686338e-01  7.24203796e-01  0.00000000e+00\n",
      "  7.93750020e-01  6.91364633e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  6.83538554e-01  0.00000000e+00  0.00000000e+00\n",
      "  5.89628611e-01 -4.67705620e-02  0.00000000e+00 -4.03797460e-02\n",
      "  1.38539684e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  5.41570518e-01 -2.75561845e-01  5.78974435e-01 -1.10003749e-01\n",
      "  1.44517467e-01 -1.71376046e-01  7.22792711e-01  2.20978014e-01\n",
      "  0.00000000e+00  7.13079351e-01 -6.22578388e-02  0.00000000e+00\n",
      "  0.00000000e+00  5.68485727e-01  0.00000000e+00  0.00000000e+00\n",
      "  4.14808053e-01  0.00000000e+00  6.92240525e-01  3.44916021e-01\n",
      "  7.16996872e-01  9.74900552e-02 -1.60688355e-01 -1.24568028e-01\n",
      "  3.13300446e-01  0.00000000e+00  6.83357527e-01 -4.53830862e-02\n",
      "  0.00000000e+00  0.00000000e+00  1.47579013e-01  6.36826816e-01\n",
      "  3.39662593e-01  0.00000000e+00  2.42132667e-01  3.95343657e-02\n",
      " -1.51124721e-01  0.00000000e+00  3.70335666e-01  0.00000000e+00\n",
      "  0.00000000e+00  3.89216511e-01  0.00000000e+00  0.00000000e+00\n",
      "  1.99076813e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.60466083e-01  2.01708016e-01 -1.90503768e-01\n",
      "  5.99916915e-01  2.28534335e-01  6.32922623e-01 -2.51826555e-02\n",
      "  4.29351909e-01 -2.34328394e-02  0.00000000e+00  0.00000000e+00\n",
      "  6.62770032e-01 -3.27421180e-01  0.00000000e+00 -1.43340232e-01\n",
      "  0.00000000e+00  0.00000000e+00  6.68597919e-01  3.67979292e-01\n",
      "  0.00000000e+00  0.00000000e+00  5.89135367e-01  0.00000000e+00\n",
      "  0.00000000e+00  6.07706874e-01  6.34785996e-01  0.00000000e+00\n",
      " -1.32145696e-01 -3.68782742e-01  6.48421894e-01  6.94136002e-01\n",
      "  0.00000000e+00  4.79146209e-01  7.36318905e-01  0.00000000e+00\n",
      " -2.11244337e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -1.35976350e-02 -7.26141362e-02 -1.67238834e-01  5.15883926e-01\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  8.53363401e-01  0.00000000e+00  6.75908136e-01\n",
      " -3.40915224e-01  3.20670963e-01  0.00000000e+00  7.82983518e-01\n",
      "  0.00000000e+00 -3.84197401e-01  3.24927409e-01  6.51462455e-01\n",
      "  5.13451329e-01  4.36190076e-01 -2.17349391e-02  5.46846989e-01\n",
      "  4.50676671e-01  3.40171063e-01  2.45848018e-01 -1.10721336e-01\n",
      " -1.99284219e-01 -9.49901568e-02  0.00000000e+00  8.23294728e-01\n",
      "  7.29734885e-01  0.00000000e+00  5.42841215e-01  6.33171147e-02\n",
      "  0.00000000e+00  2.37730053e-01  5.17204657e-01  0.00000000e+00\n",
      "  2.32126832e-01  7.26325902e-01 -1.89292687e-01  4.93600913e-01\n",
      "  3.91178499e-01 -1.23708791e-01  7.77763169e-01 -3.69098600e-01\n",
      " -1.00322616e-01  6.88931160e-01  0.00000000e+00  0.00000000e+00\n",
      "  2.57876443e-01  5.09276963e-01  6.32420177e-01  3.87945969e-01\n",
      "  6.50400693e-01 -3.26651758e-01  4.68198912e-01  6.59643357e-01\n",
      " -3.72494250e-01 -3.36188401e-01 -2.38498667e-02  2.60745507e-01\n",
      " -6.51554156e-02 -6.18972023e-02 -2.39227913e-01  3.40180610e-01\n",
      "  3.27728973e-01 -2.58996183e-01  4.23049868e-01  4.55796760e-01\n",
      "  2.39669186e-01  5.30031739e-01  6.76278209e-01  6.14360189e-01\n",
      "  7.35491756e-01  3.45127981e-01  0.00000000e+00  0.00000000e+00\n",
      "  2.07949783e-01  1.52354512e-01  6.51748194e-01  3.21493146e-01\n",
      "  3.40944316e-01  4.39797566e-01  4.66518576e-01  0.00000000e+00\n",
      "  7.28373874e-01  0.00000000e+00  6.31981269e-01  6.42313462e-01\n",
      "  4.11831998e-01  0.00000000e+00  7.06795792e-01 -2.91518147e-01\n",
      " -2.88788240e-01  5.03257428e-01  3.32974856e-01 -7.88203903e-02\n",
      " -2.11111659e-01  7.08951184e-01 -3.35063634e-01  3.91909516e-01\n",
      " -2.95858412e-01  7.07085548e-01  2.35961995e-01  0.00000000e+00\n",
      "  4.42380651e-01  5.84172046e-01  0.00000000e+00  2.17820121e-01\n",
      "  6.17538694e-01  2.68468461e-01 -2.45533019e-01  2.82684551e-01\n",
      "  7.78717321e-01  1.40454966e-02  5.55057474e-01  5.17817435e-01\n",
      "  0.00000000e+00  5.94462730e-01  9.56587519e-02  7.15367994e-01\n",
      "  4.55028509e-01 -1.67009373e-01 -2.69412585e-01  0.00000000e+00\n",
      "  6.18826377e-01  3.07894830e-01  5.47422082e-01  3.89152033e-01\n",
      "  7.16695259e-01  0.00000000e+00  2.90020477e-01  5.86514947e-01\n",
      "  5.07489554e-01  6.55429755e-01  2.27326473e-02  5.47613397e-01\n",
      "  3.16617019e-01  2.77460759e-01 -1.80019546e-01  0.00000000e+00\n",
      "  4.90572852e-01  0.00000000e+00  0.00000000e+00  6.92952528e-01\n",
      "  0.00000000e+00  0.00000000e+00 -6.68798273e-02  0.00000000e+00\n",
      "  4.61092623e-01  3.53628267e-01  2.12366966e-01  0.00000000e+00\n",
      " -1.64771520e-01  1.72494954e-01 -7.85548732e-02  9.42996044e-02\n",
      "  0.00000000e+00  3.40452512e-01  3.21852109e-01 -4.74901321e-02\n",
      "  0.00000000e+00  0.00000000e+00 -3.41841852e-01  6.41769393e-01\n",
      "  2.56987365e-02 -1.32113283e-01  7.97494142e-01  1.94496511e-01\n",
      "  5.82373204e-01  7.31477937e-01  4.19645822e-01 -1.32928601e-01\n",
      " -2.59440553e-01 -2.56577851e-01  4.60149180e-01  6.24598610e-01\n",
      "  4.31995157e-01  0.00000000e+00 -1.58772540e-01 -1.89778239e-01\n",
      "  3.72337044e-01  6.15854058e-01  5.16580929e-01 -1.30943550e-01\n",
      "  4.51785675e-01 -2.55492400e-01  6.33073959e-01  7.11474412e-01\n",
      "  2.89207709e-01  4.55355448e-01  5.31381220e-01  3.04606685e-01\n",
      " -1.42748531e-01  7.15311417e-01  6.28865710e-01 -1.81321665e-01\n",
      "  1.81303909e-01  4.37441214e-01  0.00000000e+00  3.76797773e-02\n",
      " -4.11413484e-02 -3.48254607e-01  6.19764502e-01  0.00000000e+00\n",
      "  0.00000000e+00  4.25297217e-01  3.11420709e-01  2.51685594e-01\n",
      "  0.00000000e+00  0.00000000e+00  5.55625540e-01 -1.70659931e-01\n",
      "  4.61762943e-01  4.33754329e-01 -4.78692977e-02  1.67043450e-01\n",
      "  1.89062074e-01 -3.77821022e-01 -1.87185897e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.55926882e-01  1.40214221e-01\n",
      " -1.21725264e-01  1.67926576e-01 -1.45754306e-01] [1.58 9.2  8.77 0.   0.95 8.75 9.17 1.23 0.   0.   1.03 8.42 0.58 7.78\n",
      " 1.38 0.   9.57 0.95 9.47 8.05 6.83 0.6  9.7  0.   0.   9.02 1.28 1.18\n",
      " 9.4  0.87 8.47 8.72 5.   0.   0.   0.   0.   8.05 0.   0.   8.27 0.\n",
      " 9.55 0.   6.03 6.73 3.17 0.63 0.   0.   0.35 0.87 7.37 0.   1.45 0.\n",
      " 0.52 4.1  8.42 8.97 1.08 0.   8.82 0.   0.   9.17 5.9  2.38 3.57 6.18\n",
      " 2.47 0.   0.   4.2  0.   0.   0.   2.   1.12 1.17 0.6  0.   0.   8.05\n",
      " 0.   0.   8.48 5.07 8.02 0.   0.   5.95 5.4  3.57 6.98 5.9  0.   0.\n",
      " 5.9  7.05 3.97 0.   0.   0.   0.   0.   0.98 0.4  0.   0.48 0.   0.\n",
      " 6.35 0.   1.88 2.2  3.65 0.   0.   0.7  0.   0.   6.67 2.88 8.1  0.\n",
      " 7.07 0.   0.   8.87 7.85 0.   7.88 1.75 9.47 6.43 0.   0.   2.47 0.\n",
      " 9.52 5.63 2.38 0.   5.53 0.   7.58 7.15 4.85 8.42 0.   2.25 0.   0.\n",
      " 7.3  0.   4.42 8.27 4.62 0.   0.   7.32 0.   7.62 1.12 9.05 0.   0.\n",
      " 7.5  7.1  6.38 2.38 0.   0.   1.73 8.33 0.   0.   0.   4.28 2.57 0.\n",
      " 3.65 0.   2.58 0.   8.98 0.   0.   8.17 5.47 7.47 0.   0.   0.   7.02\n",
      " 6.27 3.48 0.   2.67 3.33 0.   5.8  6.27 0.   0.   3.33 3.72 0.   3.33\n",
      " 5.8  0.4  5.68 0.   1.67 0.   0.   0.   0.   1.9  7.07 4.42 0.   1.9\n",
      " 3.4  0.   0.   0.   7.25 6.67 6.   0.   6.47 3.02 9.35 8.33 9.02 5.87\n",
      " 0.   6.67 4.17 0.   3.78 0.   0.   7.15 0.   0.   0.   0.   8.65 0.\n",
      " 6.75 6.33 0.   5.25 5.95 6.42 0.   3.75 1.9  4.28 3.73 0.   0.   8.97\n",
      " 6.67 0.   0.   0.   7.07 5.48 0.   0.   5.6  0.   0.   0.   5.42 0.\n",
      " 2.88 6.5  7.47 0.   7.85 8.83 0.   6.58 0.   0.   0.   0.   0.   1.3\n",
      " 0.   6.58 7.53 8.5  9.08 4.05 2.37 6.3  0.   0.   1.13 0.   0.   3.05\n",
      " 0.   0.92 0.   3.2  3.33 5.   1.13 2.3  0.   0.   0.   6.3  4.37 0.\n",
      " 5.23 5.8  0.   5.37 0.   3.82 1.08 0.   4.83 5.6  5.   0.   4.   8.33\n",
      " 0.   4.77 8.33 0.   5.15 2.88 1.9  5.65 5.38 0.   0.   0.   7.85 0.\n",
      " 7.7  1.42 9.02 1.27 4.93 0.   2.62 6.9  0.   0.   0.   0.   0.   6.03\n",
      " 0.   3.47 5.83 5.53 0.   4.87 0.   6.75 1.67 2.27 0.   2.27 7.85 5.43\n",
      " 0.   0.   5.47 7.03 0.   3.65 0.   9.02 0.   0.   0.   0.   0.   7.27\n",
      " 0.   0.   0.   2.72 6.08 0.   0.   7.12 5.58 4.68 0.   7.83 5.28 0.\n",
      " 0.   5.7  3.63 2.98 0.   0.   5.   5.6  0.   2.57 1.97 4.92 0.   0.98\n",
      " 7.42 3.77 4.13 1.95 8.73 0.   3.62 0.   6.07 0.   8.78 0.   0.   0.\n",
      " 0.   3.93 3.1  1.38 5.35 0.   0.   0.   0.   4.13 0.   4.87 1.9  3.88\n",
      " 0.   6.83 0.   1.43 1.32 1.22 1.28 0.   0.   0.   0.   6.28 5.13 2.88\n",
      " 3.55 0.   0.   0.   5.22 2.35 4.25 0.   0.   2.63 5.15 2.37 2.35 5.08\n",
      " 0.   7.22 4.35 7.05 1.45 6.12 3.65 2.78 5.08 7.15 0.   0.   3.25 0.\n",
      " 0.   0.   0.   5.55 2.62 1.67 3.42 0.   0.   0.   0.   0.   7.25 8.02\n",
      " 4.25 0.   0.   2.07 7.95 5.   0.   4.68 7.68 4.13 4.85 0.   5.08 5.63\n",
      " 1.88 0.   0.   2.53 0.   0.   0.   0.   0.   0.   5.98 0.   0.   0.\n",
      " 0.   2.47 0.   0.   0.   0.   2.53 6.5  5.72 8.87 5.23 7.27 5.92 5.48\n",
      " 2.2  0.   3.55 0.   2.1  3.42 6.47 0.   4.33 3.33 0.95 4.45 0.   3.97\n",
      " 0.   2.47 0.   0.   6.47 0.   0.   0.   7.07 7.47 0.   0.   6.75 0.\n",
      " 0.   0.   3.97 0.   5.08 0.   0.   3.17 3.33 5.8  3.12 6.88 5.4  0.\n",
      " 5.48 6.03 3.   5.9  8.88 7.15 0.   0.   5.45 7.65 8.18 4.85 0.   2.58\n",
      " 0.   0.   5.65 5.6  0.   0.   3.57 2.07 0.   2.62 2.92 0.   0.   5.68\n",
      " 0.   4.72 6.18 0.   0.   0.   0.   0.   5.43 4.58 0.   0.   0.   0.\n",
      " 0.   0.   1.03 0.   0.   5.35 0.   0.   3.73 0.   0.   1.95 0.   0.\n",
      " 4.05 0.   3.33 0.   0.   0.   4.17 0.   1.22 0.   7.3  6.75 1.67 0.\n",
      " 0.   0.   0.   4.   3.03 0.   3.65 5.   0.   0.   0.   3.5  0.   0.\n",
      " 5.63 7.93 0.   1.67 3.83 0.   0.   0.   8.7  2.95 3.72 4.93 4.8  3.42\n",
      " 2.38 1.58 0.   6.08 3.72 0.   0.   2.85 0.   0.   7.42 0.   2.38 2.2\n",
      " 1.3  5.95 5.6  8.73 4.1  0.   1.58 2.58 0.   0.   2.75 2.45 0.48 0.\n",
      " 1.08 0.4  3.23 0.   0.98 0.   0.   1.08 0.   0.   0.3  0.   0.   0.\n",
      " 0.   0.4  0.48 0.48 0.48 0.4  0.68 0.3  0.48 0.68 0.   0.   1.18 0.58\n",
      " 0.   0.58 0.   0.   0.4  0.68 0.   0.   0.48 0.   0.   0.58 1.18 0.\n",
      " 0.48 0.68 2.35 0.3  0.   0.68 0.48 0.   0.68 0.   0.   0.   0.3  2.65\n",
      " 0.48 0.4  0.   0.   0.   0.   0.   2.42 0.   8.17 8.25 9.32 0.   1.75\n",
      " 0.   8.2  9.13 5.9  8.72 0.83 6.87 5.8  0.95 7.62 7.33 1.58 8.57 8.33\n",
      " 0.   9.25 1.47 0.   7.1  8.1  0.   7.9  5.13 0.   0.77 5.92 1.6  6.58\n",
      " 7.22 7.47 5.08 6.88 8.87 6.75 0.   0.   0.83 0.55 7.63 8.1  5.5  7.3\n",
      " 7.62 3.07 2.7  0.23 1.33 3.98 1.75 7.05 1.05 1.08 5.68 2.3  4.92 8.82\n",
      " 7.7  2.53 4.   5.23 1.18 8.17 0.   0.   3.75 4.37 3.02 8.03 4.2  1.05\n",
      " 8.1  0.   3.62 0.   3.88 0.92 8.48 0.   4.45 0.92 5.13 4.23 7.85 1.43\n",
      " 5.8  1.98 8.73 6.28 7.63 8.47 7.67 0.   5.15 6.25 0.   6.43 4.42 3.33\n",
      " 1.28 3.82 2.42 9.08 4.7  2.47 0.   8.57 7.75 2.98 4.83 6.47 0.77 0.\n",
      " 4.08 4.85 4.17 5.   4.77 0.   7.5  3.65 4.68 8.27 1.73 5.4  4.1  3.72\n",
      " 4.13 0.   8.33 0.   0.   1.6  0.   0.   1.07 0.   4.78 6.23 3.17 0.\n",
      " 1.17 8.1  6.67 2.32 0.   4.8  6.38 2.25 0.   0.   8.02 4.67 2.22 2.63\n",
      " 5.8  3.88 1.37 5.3  2.63 4.68 4.   8.82 5.8  5.   5.75 0.   0.63 4.78\n",
      " 0.77 2.62 4.77 4.42 2.75 3.57 1.05 5.15 2.97 5.48 6.38 0.97 0.63 7.42\n",
      " 5.08 6.67 5.73 8.1  0.   1.58 7.75 3.   0.97 0.   0.   4.92 2.62 2.55\n",
      " 0.   0.   0.88 4.9  5.18 0.98 1.77 7.15 0.4  0.98 0.68 0.   0.   0.\n",
      " 2.85 1.67 4.8  2.97 6.08]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n,m=df1.shape\n",
    "df1=np.array(df1)\n",
    "cos_score=np.zeros(n)\n",
    "sim_score=np.zeros(n)\n",
    "\n",
    "c=0\n",
    "for w1,w2,simscore in df1:\n",
    "    #print(w1,w2)\n",
    "    if w1 in vocabulary and w2 in vocabulary:\n",
    "        i=word2idx[w1]\n",
    "        j=word2idx[w2]\n",
    "        cos_score[c]=cosineSimilarity(W[i],W[j])\n",
    "        sim_score[c]=simscore\n",
    "\n",
    "    c+=1\n",
    "print(cos_score,sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.38946126183732654, pvalue=1.5725898314213434e-37)\n",
      "(0.40529248109226645, 8.800378740446621e-41)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "print(stats.spearmanr(cos_score, sim_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
